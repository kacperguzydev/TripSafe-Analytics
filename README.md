# TripSafe-Analytics# ğŸš€ TripSafe Analytics

**TripSafe Analytics** is an end-to-end real-time ride-hailing analytics platform.  
It ingests streaming trip events from Kafka, processes them in Spark for both fraud detection and rolling metrics, writes results to PostgreSQL, exposes Prometheus metrics, and visualizes alerts & KPIs in Streamlit. Orchestrated with Apache Airflow and containerized via Docker.

---

## ğŸ—‚ï¸ Repository Structure

TripSafe-Analytics/
â”œâ”€â”€ airflow/ # Docker-Compose & Airflow config
â”‚ â”œâ”€â”€ dags/ # Airflow DAGs (tripsafe_pipeline.py)
â”‚ â”œâ”€â”€ logs/
â”‚ â””â”€â”€ plugins/
â”œâ”€â”€ config.py # central DB & Kafka settings
â”œâ”€â”€ database.py # create DB + tables
â”œâ”€â”€ producer.py # Kafka trip event simulator
â”œâ”€â”€ streaming.py # Spark streaming â†’ trip_events table
â”œâ”€â”€ fraud_detector.py # batch Spark fraud detector (via Airflow DAG)
â”œâ”€â”€ trip_metrics.py # batch Spark metrics job (via Airflow DAG)
â”œâ”€â”€ monitoring.py # Prometheus instrumentation
â”œâ”€â”€ dashboard.py # Streamlit dashboard (alerts & metrics)
â””â”€â”€ README.md # you are here

markdown
Copy
Edit

---

## âš™ï¸ Tech Stack

- **Streaming & Batch:** Apache Spark Structured Streaming & batch  
- **Messaging:** Apache Kafka  
- **Storage:** PostgreSQL  
- **Orchestration:** Apache Airflow (CeleryExecutor + Redis broker)  
- **Monitoring:** Prometheus + Grafana (optional)  
- **Dashboard:** Streamlit  
- **Containerization:** Docker & Docker Compose  
- **Language:** Python 3.8+

---

## ğŸ”§ Prerequisites

1. **Java 8+**  
2. **Python 3.8+** (with virtualenv)  
3. **Docker & Docker Compose**  
4. **Kafka**, **Zookeeper**, **PostgreSQL** (or use bundled Docker services)  

---

## ğŸš€ Quickstart

1. **Clone & set up**  
   ```bash
   git clone https://github.com/kacperguzydev/TripSafe-Analytics.git
   cd TripSafe-Analytics
   python3 -m venv .venv && source .venv/bin/activate
   pip install -r requirements.txt
Initialize database & tables

bash
Copy
Edit
python database.py
Start Kafka & Zookeeper

bash
Copy
Edit
docker-compose up -d zookeeper kafka
Fire up Spark jobs

Streaming ingestion

bash
Copy
Edit
python streaming.py
Monitoring endpoint

bash
Copy
Edit
python monitoring.py
In another shell:
Fraud detector (batch)

bash
Copy
Edit
python fraud_detector.py
Metrics roll-up (batch)

bash
Copy
Edit
python trip_metrics.py
Run the producer simulator

bash
Copy
Edit
python producer.py
Launch the dashboard

bash
Copy
Edit
streamlit run dashboard.py
(Optional) Orchestrate with Airflow

bash
Copy
Edit
cd airflow
docker-compose up -d
# Place tripsafe_pipeline.py in airflow/dags/
# Visit http://localhost:8080 to trigger DAG
ğŸ“ˆ Whatâ€™s Inside
producer.py â€“ simulates realistic trips + injects 2â€“5% fraud

streaming.py â€“ Spark Structured Streaming â†’ trip_events table

fraud_detector.py â€“ Spark batch job, applies rules (static GPS, short trip, jump) â†’ trip_alerts

trip_metrics.py â€“ Spark batch job, calculates daily city KPIs â†’ trip_metrics

monitoring.py â€“ exposes Prometheus metrics (/metrics on port 8002)

dashboard.py â€“ Streamlit UI for alerts & metrics

ğŸ“Š Fraud Rules
Static GPS â€“ start and end at same coordinates

Short Trip â€“ duration < 60 sec

Sudden Jump â€“ lat/lon delta > 1Â°

ğŸ“ Roadmap
âœ… Core streaming & persistence

âœ… Batch fraud + metrics jobs

âœ… Airflow orchestration

âœ… Prometheus monitoring

âœ… Streamlit dashboard
